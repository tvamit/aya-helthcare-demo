<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Aya Healthcare Voice Assistant</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 50px auto;
            padding: 20px;
            background-color: #f5f5f5;
        }
        .container {
            background: white;
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        h1 {
            color: #333;
            text-align: center;
        }
        .controls {
            display: flex;
            gap: 10px;
            justify-content: center;
            margin: 20px 0;
            flex-wrap: wrap;
        }
        button {
            padding: 12px 24px;
            font-size: 16px;
            border: none;
            border-radius: 5px;
            cursor: pointer;
            transition: background-color 0.3s;
        }
        button:disabled {
            opacity: 0.5;
            cursor: not-allowed;
        }
        .mute-btn {
            background-color: #4CAF50;
            color: white;
            font-size: 18px;
            padding: 15px 30px;
            min-width: 150px;
        }
        .mute-btn:hover:not(:disabled) {
            background-color: #45a049;
        }
        .mute-btn.muted {
            background-color: #f44336;
        }
        .mute-btn.muted:hover:not(:disabled) {
            background-color: #da190b;
        }
        .ask-btn {
            background-color: #2196F3;
            color: white;
            font-size: 18px;
            padding: 15px 30px;
            min-width: 150px;
        }
        .ask-btn:hover:not(:disabled) {
            background-color: #0b7dda;
        }
        .ask-btn.listening {
            background-color: #FF5722;
            animation: pulse 1.5s infinite;
        }
        @keyframes pulse {
            0%, 100% { opacity: 1; }
            50% { opacity: 0.6; }
        }
        .status {
            text-align: center;
            padding: 15px;
            margin: 20px 0;
            border-radius: 5px;
            font-weight: bold;
        }
        .status.connected {
            background-color: #d4edda;
            color: #155724;
        }
        .status.disconnected {
            background-color: #f8d7da;
            color: #721c24;
        }
        .status.streaming {
            background-color: #d1ecf1;
            color: #0c5460;
        }
        .status.processing {
            background-color: #fff3cd;
            color: #856404;
        }
        .transcript {
            margin: 20px 0;
            padding: 15px;
            background-color: #f8f9fa;
            border-radius: 5px;
            border-left: 4px solid #6c757d;
        }
        .transcript p {
            margin: 0;
            color: #495057;
        }
        .info {
            margin-top: 20px;
            padding: 15px;
            background-color: #e7f3ff;
            border-left: 4px solid #2196F3;
            border-radius: 4px;
        }
        .info h3 {
            margin-top: 0;
            color: #1976D2;
        }
        .info ul {
            margin: 10px 0;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>üè• Aya Healthcare Assistant</h1>

        <div id="status" class="status disconnected">
            Connecting...
        </div>

        <div class="controls">
            <button id="muteBtn" class="mute-btn muted">üîá Muted</button>
            <button id="askBtn" class="ask-btn">ü§ñ Ask AI</button>
        </div>

        <div id="transcript" class="transcript" style="display:none;">
            <p id="transcriptText"></p>
        </div>

        <div class="info">
            <h3>How to Use:</h3>
            <ul>
                <li><strong>Option 1:</strong> Click "Muted" to unmute ‚Üí Speak continuously ‚Üí Click "Unmuted" to mute and send</li>
                <li><strong>Option 2:</strong> Click "Ask AI" button ‚Üí Speak your question ‚Üí AI responds</li>
                <li><strong>Examples:</strong> "OPD ‡§ï‡§æ ‡§∏‡§Æ‡§Ø ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à?", "What are visiting hours?", "I need emergency help"</li>
            </ul>
        </div>
    </div>

    <script>
        let ws;
        let audioContext;
        let audioQueue = [];
        let isPlaying = false;
        let recognition = null;
        let isListening = false;
        
        // Streaming audio queue for chunked TTS
        let streamingAudioQueue = [];
        let isPlayingStreamingQueue = false;
        let currentStreamingAudio = null;

        // Microphone recording variables
        let mediaStream = null;
        let mediaRecorder = null;
        let audioChunks = [];
        let isMuted = true;
        let recordingStartTime = null;

        const muteBtn = document.getElementById('muteBtn');
        const askBtn = document.getElementById('askBtn');
        const statusDiv = document.getElementById('status');
        const transcriptDiv = document.getElementById('transcript');
        const transcriptText = document.getElementById('transcriptText');

        // Initialize Speech Recognition
        function initSpeechRecognition() {
            const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;

            if (!SpeechRecognition) {
                console.error('Speech Recognition not supported');
                askBtn.disabled = true;
                askBtn.textContent = '‚ùå Not Supported';
                return;
            }

            recognition = new SpeechRecognition();
            recognition.continuous = false;
            recognition.interimResults = true;
            recognition.lang = 'hi-IN'; // Hindi by default, will auto-detect

            recognition.onstart = () => {
                console.log('üé§ Speech recognition started');
                isListening = true;
                askBtn.classList.add('listening');
                askBtn.textContent = 'üé§ Listening...';
                updateStatus('Listening to your question...', 'processing');
                transcriptDiv.style.display = 'block';
                transcriptText.textContent = 'Listening...';
            };

            recognition.onresult = (event) => {
                console.log('\n========================================');
                console.log('üé§ SPEECH RECOGNITION RESULT');
                console.log('========================================');
                console.log('Results count:', event.results.length);
                console.log('Result index:', event.resultIndex);

                const transcript = Array.from(event.results)
                    .map(result => result[0])
                    .map(result => result.transcript)
                    .join('');

                console.log('Transcript:', transcript);
                console.log('Confidence:', event.results[0][0].confidence);
                console.log('Is final:', event.results[event.results.length - 1].isFinal);

                transcriptText.textContent = `You: ${transcript}`;

                // If final result
                if (event.results[event.results.length - 1].isFinal) {
                    console.log('\n‚úÖ FINAL TRANSCRIPT - Sending to AI');
                    console.log('========================================\n');
                    sendQueryToAI(transcript);
                } else {
                    console.log('‚Üí Interim result, waiting for final...');
                    console.log('========================================\n');
                }
            };

            recognition.onerror = (event) => {
                console.error('Speech recognition error:', event.error);
                isListening = false;
                askBtn.classList.remove('listening');
                askBtn.textContent = 'ü§ñ Ask AI';
                updateStatus('Error: ' + event.error, 'disconnected');
            };

            recognition.onend = () => {
                console.log('Speech recognition ended');
                isListening = false;
                askBtn.classList.remove('listening');
                askBtn.textContent = 'ü§ñ Ask AI';
            };
        }

        // Send query to AI via WebSocket
        function sendQueryToAI(text) {
            console.log('\n========================================');
            console.log('üì§ SENDING QUERY TO AI');
            console.log('========================================');
            console.log('Text:', text);
            console.log('WebSocket state:', ws ? ws.readyState : 'null');
            console.log('WebSocket OPEN constant:', WebSocket.OPEN);

            if (!text) {
                console.error('‚ùå Cannot send query: text is empty');
                return;
            }

            if (!ws) {
                console.error('‚ùå Cannot send query: WebSocket is null');
                return;
            }

            if (ws.readyState !== WebSocket.OPEN) {
                console.error('‚ùå Cannot send query: WebSocket not open. State:', ws.readyState);
                return;
            }

            console.log('‚úÖ All checks passed, sending query...');
            updateStatus('Processing your question...', 'processing');

            // Auto-detect language (simple heuristic: check for Devanagari script)
            const hasHindi = /[\u0900-\u097F]/.test(text);
            const detectedLang = hasHindi ? 'hi' : 'en';

            const query = {
                type: 'USER_QUERY',
                text: text,
                lang: detectedLang
            };

            console.log('‚Üí Detected language:', detectedLang);
            console.log('Query object:', JSON.stringify(query, null, 2));

            try {
                ws.send(JSON.stringify(query));
                console.log('‚úÖ Query sent successfully!');
                console.log('========================================\n');
            } catch (error) {
                console.error('‚ùå Error sending query:', error);
                console.error('========================================\n');
            }
        }

        // Connect to WebSocket server
        function connectWebSocket() {
            const protocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
            const wsUrl = `${protocol}//${window.location.host}`;

            ws = new WebSocket(wsUrl);

            ws.onopen = () => {
                console.log('========================================');
                console.log('‚úÖ WebSocket CONNECTED');
                console.log('========================================');
                updateStatus('Connected', 'connected');
            };

            ws.onmessage = async (event) => {
                console.log('\n========================================');
                console.log('üì® MESSAGE RECEIVED FROM SERVER');
                console.log('========================================');
                console.log('Timestamp:', new Date().toLocaleTimeString());
                console.log('Data type:', typeof event.data);
                console.log('Data size:', event.data.length || event.data.size);

                // Check if message is text (JSON) or binary (audio blob)
                if (typeof event.data === 'string') {
                    console.log('‚Üí Processing as TEXT/JSON message');
                    console.log('‚Üí Raw data:', event.data.substring(0, 200));

                    try {
                        const message = JSON.parse(event.data);
                        console.log('‚Üí Parsed JSON:', JSON.stringify(message, null, 2));
                        console.log('‚Üí Message type:', message.type);

                        if (message.type === 'WELCOME_AUDIO') {
                            console.log('üéµ WELCOME_AUDIO message detected!');
                            await playWelcomeAudio(message.url);
                        } else if (message.type === 'AI_RESPONSE_START') {
                            console.log('ü§ñ AI_RESPONSE_START message detected!');
                            console.log('‚Üí Response text:', message.text);
                            // Show text immediately
                            transcriptText.textContent = `AI: ${message.text}`;
                            transcriptDiv.style.display = 'block';
                            updateStatus('Generating response...', 'processing');
                            
                            // Clear previous streaming queue
                            streamingAudioQueue = [];
                            isPlayingStreamingQueue = false;
                            if (currentStreamingAudio) {
                                currentStreamingAudio.pause();
                                currentStreamingAudio = null;
                            }
                        } else if (message.type === 'AI_RESPONSE_CHUNK') {
                            console.log(`üì¶ AI_RESPONSE_CHUNK received (chunk ${message.chunkId}): ${message.url}`);
                            // Add chunk to streaming queue and play if not already playing
                            streamingAudioQueue.push({
                                url: message.url,
                                chunkId: message.chunkId,
                                text: message.text
                            });
                            
                            // Start playing if queue was empty
                            if (!isPlayingStreamingQueue) {
                                playNextStreamingChunk();
                            }
                        } else if (message.type === 'AI_RESPONSE_END') {
                            console.log(`‚úÖ AI_RESPONSE_END - All ${message.totalChunks} chunks received`);
                            updateStatus('Response complete', 'connected');
                        } else if (message.type === 'AI_RESPONSE') {
                            console.log('ü§ñ AI_RESPONSE message detected! (backward compatibility)');
                            console.log('‚Üí Response text:', message.text);
                            console.log('‚Üí Response audio URL:', message.url);
                            await handleAIResponse(message);
                        } else {
                            console.log('‚ö†Ô∏è  Unknown message type:', message.type);
                        }
                    } catch (e) {
                        console.error('‚ùå Error parsing JSON message:', e);
                        console.error('‚Üí Error stack:', e.stack);
                    }
                } else {
                    // Binary audio data
                    console.log('‚Üí Processing as BINARY audio data');
                    await playAudio(event.data);
                }
                console.log('========================================\n');
            };

            ws.onclose = () => {
                console.log('\n========================================');
                console.log('‚ùå WebSocket DISCONNECTED');
                console.log('========================================');
                updateStatus('Disconnected - Reconnecting...', 'disconnected');

                setTimeout(() => {
                    console.log('‚Üí Attempting to reconnect...');
                    connectWebSocket();
                }, 3000);
            };

            ws.onerror = (error) => {
                console.error('WebSocket error:', error);
            };
        }

        // Handle AI response (backward compatibility for single audio file)
        async function handleAIResponse(message) {
            console.log('AI Response:', message.text);

            // Show response in transcript
            transcriptText.textContent = `AI: ${message.text}`;

            // Play audio if available
            if (message.url) {
                await playWelcomeAudio(message.url);
            }

            updateStatus('In Call (Muted)', 'connected');
        }

        // Enhanced audio queue player for streaming chunks
        async function playNextStreamingChunk() {
            if (streamingAudioQueue.length === 0) {
                isPlayingStreamingQueue = false;
                console.log('‚úÖ Streaming audio queue finished');
                return;
            }

            isPlayingStreamingQueue = true;
            const chunk = streamingAudioQueue.shift();
            
            console.log(`‚ñ∂Ô∏è  Playing streaming chunk ${chunk.chunkId}: ${chunk.url}`);
            
            try {
                const audio = new Audio(chunk.url);
                currentStreamingAudio = audio;
                
                audio.onloadeddata = () => {
                    console.log(`‚Üí Chunk ${chunk.chunkId} loaded, duration: ${audio.duration}s`);
                };
                
                audio.onended = () => {
                    console.log(`‚úÖ Chunk ${chunk.chunkId} finished`);
                    currentStreamingAudio = null;
                    // Play next chunk immediately
                    playNextStreamingChunk();
                };
                
                audio.onerror = (error) => {
                    console.error(`‚ùå Error playing chunk ${chunk.chunkId}:`, error);
                    currentStreamingAudio = null;
                    // Continue with next chunk even if this one fails
                    playNextStreamingChunk();
                };
                
                // Play the audio
                await audio.play();
                
            } catch (error) {
                console.error(`‚ùå Error loading chunk ${chunk.chunkId}:`, error);
                currentStreamingAudio = null;
                // Continue with next chunk
                playNextStreamingChunk();
            }
        }

        // Update status display
        function updateStatus(message, className) {
            statusDiv.textContent = message;
            statusDiv.className = `status ${className}`;
        }

        // Initialize microphone for recording
        async function initMicrophone() {
            console.log('\n========================================');
            console.log('üé§ INITIALIZING MICROPHONE');
            console.log('========================================');

            try {
                mediaStream = await navigator.mediaDevices.getUserMedia({
                    audio: {
                        channelCount: 1,
                        sampleRate: 16000,
                        echoCancellation: true,
                        noiseSuppression: true
                    }
                });

                console.log('‚úÖ Microphone access granted');
                console.log('‚Üí Audio tracks:', mediaStream.getAudioTracks().length);

                // Check supported MIME types
                let mimeType = 'audio/webm;codecs=opus';

                if (!MediaRecorder.isTypeSupported(mimeType)) {
                    console.log('‚ö†Ô∏è  audio/webm;codecs=opus not supported, trying audio/webm');
                    mimeType = 'audio/webm';
                }

                if (!MediaRecorder.isTypeSupported(mimeType)) {
                    console.error('‚ùå No supported audio format found');
                    return;
                }

                console.log('‚Üí Using MIME type:', mimeType);

                mediaRecorder = new MediaRecorder(mediaStream, {
                    mimeType: mimeType
                });

                // Track if we're waiting for final chunks
                let isProcessingStop = false;
                let stopProcessingTimeout = null;

                mediaRecorder.ondataavailable = (event) => {
                    console.log('‚Üí Audio chunk received:', event.data.size, 'bytes');
                    if (event.data.size > 0) {
                        audioChunks.push(event.data);
                        console.log('‚Üí Total chunks so far:', audioChunks.length);
                        console.log('‚Üí Chunk added, total size:', audioChunks.reduce((sum, chunk) => sum + chunk.size, 0), 'bytes');
                    } else {
                        console.warn('‚ö†Ô∏è  Received empty chunk');
                    }

                    // If we're processing stop, clear the timeout and process
                    if (isProcessingStop && stopProcessingTimeout) {
                        clearTimeout(stopProcessingTimeout);
                        stopProcessingTimeout = setTimeout(() => {
                            processRecordedAudio();
                        }, 100); // Wait 100ms for any remaining chunks
                    }
                };

                // Function to process the recorded audio
                async function processRecordedAudio() {
                    if (stopProcessingTimeout) {
                        clearTimeout(stopProcessingTimeout);
                        stopProcessingTimeout = null;
                    }
                    isProcessingStop = false;

                    console.log('\n========================================');
                    console.log('üé§ PROCESSING RECORDED AUDIO');
                    console.log('========================================');
                    console.log('‚Üí Total chunks collected:', audioChunks.length);
                    console.log('‚Üí Recording duration:', recordingStartTime ? (Date.now() - recordingStartTime) + 'ms' : 'unknown');

                    if (audioChunks.length === 0) {
                        console.log('‚ö†Ô∏è  No audio chunks collected');
                        updateStatus('No audio recorded. Please try again.', 'disconnected');
                        recordingStartTime = null;
                        return;
                    }

                    // Calculate total size before creating blob
                    let totalSize = 0;
                    audioChunks.forEach((chunk, index) => {
                        totalSize += chunk.size;
                        console.log(`‚Üí Chunk ${index + 1} size:`, chunk.size, 'bytes');
                    });
                    console.log('‚Üí Total audio data size:', totalSize, 'bytes');

                    const audioBlob = new Blob(audioChunks, { type: mimeType });
                    console.log('‚Üí Audio blob created');
                    console.log('‚Üí Blob size:', audioBlob.size, 'bytes');
                    console.log('‚Üí Blob type:', audioBlob.type);

                    const chunksToSend = [...audioChunks]; // Copy for sending
                    audioChunks = [];
                    recordingStartTime = null; // Reset

                    // Lower minimum: 500 bytes (allows short questions like "OPD ‡§ï‡§æ ‡§∏‡§Æ‡§Ø ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à?")
                    if (audioBlob.size < 500) {
                        console.log('‚ö†Ô∏è  Audio too small (< 500 bytes), not sending');
                        console.log('‚Üí This might indicate a recording issue. Please check microphone permissions.');
                        console.log('‚Üí Chunks collected:', chunksToSend.length);
                        updateStatus('Audio too small. Please check microphone and try again.', 'disconnected');
                        return;
                    }

                    // Convert to ArrayBuffer and send
                    console.log('‚Üí Converting to ArrayBuffer...');
                    const arrayBuffer = await audioBlob.arrayBuffer();
                    console.log('‚Üí ArrayBuffer size:', arrayBuffer.byteLength, 'bytes');

                    if (ws && ws.readyState === WebSocket.OPEN) {
                        console.log('‚Üí Sending audio to server...');
                        updateStatus('Processing your question...', 'processing');
                        ws.send(arrayBuffer);
                        console.log('‚úÖ Audio sent successfully!');
                        console.log('========================================\n');
                    } else {
                        console.error('‚ùå WebSocket not connected');
                        updateStatus('Connection error. Please try again.', 'disconnected');
                    }
                }

                mediaRecorder.onstop = () => {
                    console.log('\n========================================');
                    console.log('üé§ RECORDING STOPPED EVENT');
                    console.log('========================================');
                    console.log('‚Üí MediaRecorder state:', mediaRecorder.state);
                    console.log('‚Üí Chunks collected so far:', audioChunks.length);
                    
                    // Request final data chunk
                    if (mediaRecorder.state !== 'inactive') {
                        console.log('‚Üí Requesting final data...');
                        mediaRecorder.requestData();
                    }

                    // Mark that we're processing stop and wait for final chunks
                    isProcessingStop = true;
                    
                    // Wait a bit for all ondataavailable events to fire
                    // Then process the audio
                    stopProcessingTimeout = setTimeout(() => {
                        processRecordedAudio();
                    }, 200); // Wait 200ms for all chunks to arrive
                };

                muteBtn.disabled = false;
                console.log('‚úÖ MediaRecorder initialized');
                console.log('========================================\n');

            } catch (error) {
                console.error('‚ùå Microphone initialization error:', error);
                muteBtn.disabled = true;
                muteBtn.textContent = '‚ùå Mic Error';
            }
        }

        // Handle mute/unmute button
        function handleMuteButton() {
            console.log('\n========================================');
            console.log('üîò MUTE BUTTON CLICKED');
            console.log('========================================');
            console.log('‚Üí Current state: isMuted =', isMuted);

            if (!mediaRecorder) {
                console.error('‚ùå MediaRecorder not initialized');
                return;
            }

            if (isMuted) {
                // Unmute - Start recording
                console.log('‚Üí Action: UNMUTING (start recording)');
                audioChunks = [];
                recordingStartTime = Date.now();

                try {
                    // Start recording with timeslice to collect chunks during recording
                    // This ensures we collect data even if stop() is called quickly
                    mediaRecorder.start(100); // Get chunks every 100ms
                    console.log('‚Üí MediaRecorder.start() called (with 100ms timeslice)');
                    console.log('‚Üí MediaRecorder state:', mediaRecorder.state);

                    isMuted = false;
                    muteBtn.classList.remove('muted');
                    muteBtn.textContent = 'üé§ Unmuted';
                    updateStatus('Recording... Speak now!', 'streaming');
                    console.log('‚úÖ Recording started');
                } catch (error) {
                    console.error('‚ùå Error starting recording:', error);
                }
            } else {
                // Mute - Stop recording and send
                console.log('‚Üí Action: MUTING (stop recording and send)');
                
                const recordingDuration = recordingStartTime ? (Date.now() - recordingStartTime) : 0;
                console.log('‚Üí Recording duration:', recordingDuration, 'ms');
                console.log('‚Üí Current chunks before stop:', audioChunks.length);

                // Lower minimum: 500ms (0.5 seconds) - allows short questions
                if (recordingDuration < 500) {
                    console.log('‚ö†Ô∏è  Recording too short (< 500ms), please try again');
                    updateStatus('Recording too short. Please try again.', 'disconnected');
                    
                    // Reset state
                    isMuted = true;
                    muteBtn.classList.add('muted');
                    muteBtn.textContent = 'üîá Muted';
                    audioChunks = [];
                    recordingStartTime = null;
                    
                    // Stop the recorder if it's recording
                    if (mediaRecorder.state === 'recording') {
                        try {
                            mediaRecorder.stop();
                        } catch (e) {
                            console.error('Error stopping recorder:', e);
                        }
                    }
                    
                    return;
                }

                try {
                    // Request data before stopping to ensure we get the final chunk
                    if (mediaRecorder.state === 'recording') {
                        console.log('‚Üí Requesting data before stop...');
                        mediaRecorder.requestData();
                    }
                    
                    // Small delay to ensure requestData is processed
                    setTimeout(() => {
                        mediaRecorder.stop();
                        console.log('‚Üí MediaRecorder.stop() called');
                        console.log('‚Üí MediaRecorder state:', mediaRecorder.state);
                        console.log('‚Üí Recording duration:', recordingDuration, 'ms');

                        isMuted = true;
                        muteBtn.classList.add('muted');
                        muteBtn.textContent = 'üîá Muted';
                        console.log('‚úÖ Recording stopped, waiting for all chunks...');
                    }, 50); // Small delay to process requestData
                } catch (error) {
                    console.error('‚ùå Error stopping recording:', error);
                    isMuted = true;
                    muteBtn.classList.add('muted');
                    muteBtn.textContent = 'üîá Muted';
                }
            }

            console.log('========================================\n');
        }

        // Ask AI button handler
        function handleAskButton() {
            if (!recognition) {
                alert('Speech recognition not available');
                return;
            }

            if (isListening) {
                recognition.stop();
            } else {
                recognition.start();
            }
        }

        // Play welcome audio
        async function playWelcomeAudio(audioUrl) {
            console.log('\nüîä Playing audio:', audioUrl);

            try {
                const audio = new Audio(audioUrl);

                audio.onloadeddata = () => {
                    console.log('‚Üí Audio loaded, duration:', audio.duration);
                };

                audio.onerror = (error) => {
                    console.error('‚ùå Audio error:', error);
                };

                audio.onended = () => {
                    console.log('‚úÖ Audio finished');
                };

                const playPromise = audio.play();

                if (playPromise !== undefined) {
                    playPromise
                        .then(() => console.log('‚úÖ Audio playing'))
                        .catch((error) => {
                            console.error('‚ùå Autoplay blocked:', error.message);
                        });
                }
            } catch (error) {
                console.error('‚ùå Playback error:', error);
            }
        }

        // Play received audio chunks
        async function playAudio(audioData) {
            try {
                if (!audioContext) {
                    audioContext = new (window.AudioContext || window.webkitAudioContext)();
                }

                const arrayBuffer = await audioData.arrayBuffer();
                const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);

                audioQueue.push(audioBuffer);

                if (!isPlaying) {
                    playNextInQueue();
                }
            } catch (error) {
                console.error('Error playing audio:', error);
            }
        }

        // Play next audio in queue
        function playNextInQueue() {
            if (audioQueue.length === 0) {
                isPlaying = false;
                return;
            }

            isPlaying = true;
            const audioBuffer = audioQueue.shift();

            const source = audioContext.createBufferSource();
            source.buffer = audioBuffer;
            source.connect(audioContext.destination);

            source.onended = () => {
                playNextInQueue();
            };

            source.start(0);
        }

        // Event listeners
        muteBtn.addEventListener('click', handleMuteButton);
        askBtn.addEventListener('click', handleAskButton);

        // Initialize everything
        function initialize() {
            console.log('üöÄ Initializing Aya Healthcare Assistant...');
            connectWebSocket();
            initSpeechRecognition();
            initMicrophone();
            console.log('‚úÖ Initialization complete!');
        }

        // Start on page load
        initialize();
    </script>
</body>
</html>
