<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Aya Healthcare Voice Assistant</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 50px auto;
            padding: 20px;
            background-color: #f5f5f5;
        }
        .container {
            background: white;
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        h1 {
            color: #333;
            text-align: center;
        }
        .controls {
            display: flex;
            gap: 10px;
            justify-content: center;
            margin: 20px 0;
            flex-wrap: wrap;
        }
        button {
            padding: 12px 24px;
            font-size: 16px;
            border: none;
            border-radius: 5px;
            cursor: pointer;
            transition: background-color 0.3s;
        }
        button:disabled {
            opacity: 0.5;
            cursor: not-allowed;
        }
        .mute-btn {
            background-color: #4CAF50;
            color: white;
            font-size: 18px;
            padding: 15px 30px;
            min-width: 150px;
        }
        .mute-btn:hover:not(:disabled) {
            background-color: #45a049;
        }
        .mute-btn.muted {
            background-color: #f44336;
        }
        .mute-btn.muted:hover:not(:disabled) {
            background-color: #da190b;
        }
        .ask-btn {
            background-color: #2196F3;
            color: white;
            font-size: 18px;
            padding: 15px 30px;
            min-width: 150px;
        }
        .ask-btn:hover:not(:disabled) {
            background-color: #0b7dda;
        }
        .ask-btn.listening {
            background-color: #FF5722;
            animation: pulse 1.5s infinite;
        }
        @keyframes pulse {
            0%, 100% { opacity: 1; }
            50% { opacity: 0.6; }
        }
        .status {
            text-align: center;
            padding: 15px;
            margin: 20px 0;
            border-radius: 5px;
            font-weight: bold;
        }
        .status.connected {
            background-color: #d4edda;
            color: #155724;
        }
        .status.disconnected {
            background-color: #f8d7da;
            color: #721c24;
        }
        .status.streaming {
            background-color: #d1ecf1;
            color: #0c5460;
        }
        .status.processing {
            background-color: #fff3cd;
            color: #856404;
        }
        .transcript {
            margin: 20px 0;
            padding: 15px;
            background-color: #f8f9fa;
            border-radius: 5px;
            border-left: 4px solid #6c757d;
        }
        .transcript p {
            margin: 0;
            color: #495057;
        }
        .info {
            margin-top: 20px;
            padding: 15px;
            background-color: #e7f3ff;
            border-left: 4px solid #2196F3;
            border-radius: 4px;
        }
        .info h3 {
            margin-top: 0;
            color: #1976D2;
        }
        .info ul {
            margin: 10px 0;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>üè• Aya Healthcare Assistant</h1>

        <div id="status" class="status disconnected">
            Connecting...
        </div>

        <div class="controls">
            <button id="muteBtn" class="mute-btn muted">üîá Muted</button>
            <button id="askBtn" class="ask-btn">ü§ñ Ask AI</button>
        </div>

        <div id="transcript" class="transcript" style="display:none;">
            <p id="transcriptText"></p>
        </div>

        <div class="info">
            <h3>How to Use:</h3>
            <ul>
                <li><strong>Option 1:</strong> Click "Muted" to unmute ‚Üí Speak continuously ‚Üí Click "Unmuted" to mute and send</li>
                <li><strong>Option 2:</strong> Click "Ask AI" button ‚Üí Speak your question ‚Üí AI responds</li>
                <li><strong>Examples:</strong> "OPD ‡§ï‡§æ ‡§∏‡§Æ‡§Ø ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à?", "What are visiting hours?", "I need emergency help"</li>
            </ul>
        </div>
    </div>

    <script>
        let ws;
        let audioContext;
        let audioQueue = [];
        let isPlaying = false;
        let recognition = null;
        let isListening = false;

        // Microphone recording variables
        let mediaStream = null;
        let mediaRecorder = null;
        let audioChunks = [];
        let isMuted = true;

        const muteBtn = document.getElementById('muteBtn');
        const askBtn = document.getElementById('askBtn');
        const statusDiv = document.getElementById('status');
        const transcriptDiv = document.getElementById('transcript');
        const transcriptText = document.getElementById('transcriptText');

        // Initialize Speech Recognition
        function initSpeechRecognition() {
            const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;

            if (!SpeechRecognition) {
                console.error('Speech Recognition not supported');
                askBtn.disabled = true;
                askBtn.textContent = '‚ùå Not Supported';
                return;
            }

            recognition = new SpeechRecognition();
            recognition.continuous = false;
            recognition.interimResults = true;
            recognition.lang = 'hi-IN'; // Hindi by default, will auto-detect

            recognition.onstart = () => {
                console.log('üé§ Speech recognition started');
                isListening = true;
                askBtn.classList.add('listening');
                askBtn.textContent = 'üé§ Listening...';
                updateStatus('Listening to your question...', 'processing');
                transcriptDiv.style.display = 'block';
                transcriptText.textContent = 'Listening...';
            };

            recognition.onresult = (event) => {
                console.log('\n========================================');
                console.log('üé§ SPEECH RECOGNITION RESULT');
                console.log('========================================');
                console.log('Results count:', event.results.length);
                console.log('Result index:', event.resultIndex);

                const transcript = Array.from(event.results)
                    .map(result => result[0])
                    .map(result => result.transcript)
                    .join('');

                console.log('Transcript:', transcript);
                console.log('Confidence:', event.results[0][0].confidence);
                console.log('Is final:', event.results[event.results.length - 1].isFinal);

                transcriptText.textContent = `You: ${transcript}`;

                // If final result
                if (event.results[event.results.length - 1].isFinal) {
                    console.log('\n‚úÖ FINAL TRANSCRIPT - Sending to AI');
                    console.log('========================================\n');
                    sendQueryToAI(transcript);
                } else {
                    console.log('‚Üí Interim result, waiting for final...');
                    console.log('========================================\n');
                }
            };

            recognition.onerror = (event) => {
                console.error('Speech recognition error:', event.error);
                isListening = false;
                askBtn.classList.remove('listening');
                askBtn.textContent = 'ü§ñ Ask AI';
                updateStatus('Error: ' + event.error, 'disconnected');
            };

            recognition.onend = () => {
                console.log('Speech recognition ended');
                isListening = false;
                askBtn.classList.remove('listening');
                askBtn.textContent = 'ü§ñ Ask AI';
            };
        }

        // Send query to AI via WebSocket
        function sendQueryToAI(text) {
            console.log('\n========================================');
            console.log('üì§ SENDING QUERY TO AI');
            console.log('========================================');
            console.log('Text:', text);
            console.log('WebSocket state:', ws ? ws.readyState : 'null');
            console.log('WebSocket OPEN constant:', WebSocket.OPEN);

            if (!text) {
                console.error('‚ùå Cannot send query: text is empty');
                return;
            }

            if (!ws) {
                console.error('‚ùå Cannot send query: WebSocket is null');
                return;
            }

            if (ws.readyState !== WebSocket.OPEN) {
                console.error('‚ùå Cannot send query: WebSocket not open. State:', ws.readyState);
                return;
            }

            console.log('‚úÖ All checks passed, sending query...');
            updateStatus('Processing your question...', 'processing');

            // Auto-detect language (simple heuristic: check for Devanagari script)
            const hasHindi = /[\u0900-\u097F]/.test(text);
            const detectedLang = hasHindi ? 'hi' : 'en';

            const query = {
                type: 'USER_QUERY',
                text: text,
                lang: detectedLang
            };

            console.log('‚Üí Detected language:', detectedLang);
            console.log('Query object:', JSON.stringify(query, null, 2));

            try {
                ws.send(JSON.stringify(query));
                console.log('‚úÖ Query sent successfully!');
                console.log('========================================\n');
            } catch (error) {
                console.error('‚ùå Error sending query:', error);
                console.error('========================================\n');
            }
        }

        // Connect to WebSocket server
        function connectWebSocket() {
            const protocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
            const wsUrl = `${protocol}//${window.location.host}`;

            ws = new WebSocket(wsUrl);

            ws.onopen = () => {
                console.log('========================================');
                console.log('‚úÖ WebSocket CONNECTED');
                console.log('========================================');
                updateStatus('Connected', 'connected');
            };

            ws.onmessage = async (event) => {
                console.log('\n========================================');
                console.log('üì® MESSAGE RECEIVED FROM SERVER');
                console.log('========================================');
                console.log('Timestamp:', new Date().toLocaleTimeString());
                console.log('Data type:', typeof event.data);
                console.log('Data size:', event.data.length || event.data.size);

                // Check if message is text (JSON) or binary (audio blob)
                if (typeof event.data === 'string') {
                    console.log('‚Üí Processing as TEXT/JSON message');
                    console.log('‚Üí Raw data:', event.data.substring(0, 200));

                    try {
                        const message = JSON.parse(event.data);
                        console.log('‚Üí Parsed JSON:', JSON.stringify(message, null, 2));
                        console.log('‚Üí Message type:', message.type);

                        if (message.type === 'WELCOME_AUDIO') {
                            console.log('üéµ WELCOME_AUDIO message detected!');
                            await playWelcomeAudio(message.url);
                        } else if (message.type === 'AI_RESPONSE') {
                            console.log('ü§ñ AI_RESPONSE message detected!');
                            console.log('‚Üí Response text:', message.text);
                            console.log('‚Üí Response audio URL:', message.url);
                            await handleAIResponse(message);
                        } else {
                            console.log('‚ö†Ô∏è  Unknown message type:', message.type);
                        }
                    } catch (e) {
                        console.error('‚ùå Error parsing JSON message:', e);
                        console.error('‚Üí Error stack:', e.stack);
                    }
                } else {
                    // Binary audio data
                    console.log('‚Üí Processing as BINARY audio data');
                    await playAudio(event.data);
                }
                console.log('========================================\n');
            };

            ws.onclose = () => {
                console.log('\n========================================');
                console.log('‚ùå WebSocket DISCONNECTED');
                console.log('========================================');
                updateStatus('Disconnected - Reconnecting...', 'disconnected');

                setTimeout(() => {
                    console.log('‚Üí Attempting to reconnect...');
                    connectWebSocket();
                }, 3000);
            };

            ws.onerror = (error) => {
                console.error('WebSocket error:', error);
            };
        }

        // Handle AI response
        async function handleAIResponse(message) {
            console.log('AI Response:', message.text);

            // Show response in transcript
            transcriptText.textContent = `AI: ${message.text}`;

            // Play audio if available
            if (message.url) {
                await playWelcomeAudio(message.url);
            }

            updateStatus('In Call (Muted)', 'connected');
        }

        // Update status display
        function updateStatus(message, className) {
            statusDiv.textContent = message;
            statusDiv.className = `status ${className}`;
        }

        // Initialize microphone for recording
        async function initMicrophone() {
            console.log('\n========================================');
            console.log('üé§ INITIALIZING MICROPHONE');
            console.log('========================================');

            try {
                mediaStream = await navigator.mediaDevices.getUserMedia({
                    audio: {
                        channelCount: 1,
                        sampleRate: 16000,
                        echoCancellation: true,
                        noiseSuppression: true
                    }
                });

                console.log('‚úÖ Microphone access granted');
                console.log('‚Üí Audio tracks:', mediaStream.getAudioTracks().length);

                // Check supported MIME types
                let mimeType = 'audio/webm;codecs=opus';

                if (!MediaRecorder.isTypeSupported(mimeType)) {
                    console.log('‚ö†Ô∏è  audio/webm;codecs=opus not supported, trying audio/webm');
                    mimeType = 'audio/webm';
                }

                if (!MediaRecorder.isTypeSupported(mimeType)) {
                    console.error('‚ùå No supported audio format found');
                    return;
                }

                console.log('‚Üí Using MIME type:', mimeType);

                mediaRecorder = new MediaRecorder(mediaStream, {
                    mimeType: mimeType
                });

                mediaRecorder.ondataavailable = (event) => {
                    console.log('‚Üí Audio chunk received:', event.data.size, 'bytes');
                    if (event.data.size > 0) {
                        audioChunks.push(event.data);
                    }
                };

                mediaRecorder.onstop = async () => {
                    console.log('\n========================================');
                    console.log('üé§ RECORDING STOPPED');
                    console.log('========================================');
                    console.log('‚Üí Total chunks collected:', audioChunks.length);

                    if (audioChunks.length === 0) {
                        console.log('‚ö†Ô∏è  No audio chunks collected');
                        updateStatus('No audio recorded. Please try again.', 'disconnected');
                        return;
                    }

                    const audioBlob = new Blob(audioChunks, { type: mimeType });
                    console.log('‚Üí Audio blob created');
                    console.log('‚Üí Blob size:', audioBlob.size, 'bytes');
                    console.log('‚Üí Blob type:', audioBlob.type);

                    audioChunks = [];

                    // Validate audio size
                    if (audioBlob.size < 1000) {
                        console.log('‚ö†Ô∏è  Audio too short (< 1000 bytes), not sending');
                        updateStatus('Audio too short. Please speak for at least 2-3 seconds.', 'disconnected');
                        return;
                    }

                    // Convert to ArrayBuffer and send
                    console.log('‚Üí Converting to ArrayBuffer...');
                    const arrayBuffer = await audioBlob.arrayBuffer();
                    console.log('‚Üí ArrayBuffer size:', arrayBuffer.byteLength, 'bytes');

                    if (ws && ws.readyState === WebSocket.OPEN) {
                        console.log('‚Üí Sending audio to server...');
                        updateStatus('Processing your question...', 'processing');
                        ws.send(arrayBuffer);
                        console.log('‚úÖ Audio sent successfully!');
                        console.log('========================================\n');
                    } else {
                        console.error('‚ùå WebSocket not connected');
                        updateStatus('Connection error. Please try again.', 'disconnected');
                    }
                };

                muteBtn.disabled = false;
                console.log('‚úÖ MediaRecorder initialized');
                console.log('========================================\n');

            } catch (error) {
                console.error('‚ùå Microphone initialization error:', error);
                muteBtn.disabled = true;
                muteBtn.textContent = '‚ùå Mic Error';
            }
        }

        // Handle mute/unmute button
        function handleMuteButton() {
            console.log('\n========================================');
            console.log('üîò MUTE BUTTON CLICKED');
            console.log('========================================');
            console.log('‚Üí Current state: isMuted =', isMuted);

            if (!mediaRecorder) {
                console.error('‚ùå MediaRecorder not initialized');
                return;
            }

            if (isMuted) {
                // Unmute - Start recording
                console.log('‚Üí Action: UNMUTING (start recording)');
                audioChunks = [];

                try {
                    // Start recording without timeslice - collect all until stop()
                    mediaRecorder.start();
                    console.log('‚Üí MediaRecorder.start() called (no timeslice - continuous recording)');
                    console.log('‚Üí MediaRecorder state:', mediaRecorder.state);

                    isMuted = false;
                    muteBtn.classList.remove('muted');
                    muteBtn.textContent = 'üé§ Unmuted';
                    updateStatus('Recording... Speak now! (speak for at least 3 seconds)', 'streaming');
                    console.log('‚úÖ Recording started');
                } catch (error) {
                    console.error('‚ùå Error starting recording:', error);
                }
            } else {
                // Mute - Stop recording and send
                console.log('‚Üí Action: MUTING (stop recording and send)');

                try {
                    mediaRecorder.stop();
                    console.log('‚Üí MediaRecorder.stop() called');
                    console.log('‚Üí MediaRecorder state:', mediaRecorder.state);

                    isMuted = true;
                    muteBtn.classList.add('muted');
                    muteBtn.textContent = 'üîá Muted';
                    console.log('‚úÖ Recording stopped, audio will be sent when processed');
                } catch (error) {
                    console.error('‚ùå Error stopping recording:', error);
                }
            }

            console.log('========================================\n');
        }

        // Ask AI button handler
        function handleAskButton() {
            if (!recognition) {
                alert('Speech recognition not available');
                return;
            }

            if (isListening) {
                recognition.stop();
            } else {
                recognition.start();
            }
        }

        // Play welcome audio
        async function playWelcomeAudio(audioUrl) {
            console.log('\nüîä Playing audio:', audioUrl);

            try {
                const audio = new Audio(audioUrl);

                audio.onloadeddata = () => {
                    console.log('‚Üí Audio loaded, duration:', audio.duration);
                };

                audio.onerror = (error) => {
                    console.error('‚ùå Audio error:', error);
                };

                audio.onended = () => {
                    console.log('‚úÖ Audio finished');
                };

                const playPromise = audio.play();

                if (playPromise !== undefined) {
                    playPromise
                        .then(() => console.log('‚úÖ Audio playing'))
                        .catch((error) => {
                            console.error('‚ùå Autoplay blocked:', error.message);
                        });
                }
            } catch (error) {
                console.error('‚ùå Playback error:', error);
            }
        }

        // Play received audio chunks
        async function playAudio(audioData) {
            try {
                if (!audioContext) {
                    audioContext = new (window.AudioContext || window.webkitAudioContext)();
                }

                const arrayBuffer = await audioData.arrayBuffer();
                const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);

                audioQueue.push(audioBuffer);

                if (!isPlaying) {
                    playNextInQueue();
                }
            } catch (error) {
                console.error('Error playing audio:', error);
            }
        }

        // Play next audio in queue
        function playNextInQueue() {
            if (audioQueue.length === 0) {
                isPlaying = false;
                return;
            }

            isPlaying = true;
            const audioBuffer = audioQueue.shift();

            const source = audioContext.createBufferSource();
            source.buffer = audioBuffer;
            source.connect(audioContext.destination);

            source.onended = () => {
                playNextInQueue();
            };

            source.start(0);
        }

        // Event listeners
        muteBtn.addEventListener('click', handleMuteButton);
        askBtn.addEventListener('click', handleAskButton);

        // Initialize everything
        function initialize() {
            console.log('üöÄ Initializing Aya Healthcare Assistant...');
            connectWebSocket();
            initSpeechRecognition();
            initMicrophone();
            console.log('‚úÖ Initialization complete!');
        }

        // Start on page load
        initialize();
    </script>
</body>
</html>
